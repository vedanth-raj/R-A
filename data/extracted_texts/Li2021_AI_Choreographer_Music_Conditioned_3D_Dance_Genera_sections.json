{
  "sections": [
    {
      "title": "Abstract",
      "content": "We present AIST++, a new multi-modal dataset of 3D\ndance motion and music, along with FACT, a Full-Attention\nCross-modal Transformer network for generating 3D dance\nmotion conditioned on music.\nThe proposed AIST++\ndataset contains 5.2 hours of 3D dance motion in 1408 se-\nquences, covering 10 dance genres with multi-view videos\nwith known camera poses\u2014the largest dataset of this kind\nto our knowledge. We show that naively applying sequence\nmodels such as transformers to this dataset for the task of\nmusic conditioned 3D motion generation does not produce\nsatisfactory 3D motion that is well correlated with the input\nmusic. We overcome these shortcomings by introducing key\nchanges in its architecture design and supervision: FACT\nmodel involves a deep cross-modal transformer block with\nfull-attention that is trained to predict N future motions. We\nempirically show that these changes are key factors in gen-\nerating long sequences of realistic dance motion that are\nwell-attuned to the input music. We conduct extensive ex-\nperiments on AIST++ with user studies, where our method\noutperforms recent state-of-the-art methods both qualita-\ntively and quantitatively. The code and the dataset can be\nfound at: https://google.github.io/aichoreographer.\n\u2217equal contribution. Work performed while Ruilong was an intern at\nGoogle.",
      "type": "abstract",
      "word_count": 199
    },
    {
      "title": "1. Introduction",
      "content": "The ability to dance by composing movement patterns\nthat align to musical beats is a fundamental aspect of hu-\nman behavior. Dancing is an universal language found in all\ncultures [50], and today, many people express themselves\nthrough dance on contemporary online media platforms.\nThe most watched videos on YouTube are dance-centric\nmusic videos such as \u201cBaby Shark Dance\u201d, and \u201cGangnam\nStyle\u201d [75], making dance a more and more powerful tool\nto spread messages across the internet. However, dancing is\na form of art that requires practice\u2014even for humans, pro-\nfessional training is required to equip a dancer with a rich\nrepertoire of dance motions to create an expressive chore-\nography. Computationally, this is even more challenging as\nthe task requires the ability to generate a continuous motion\nwith high kinematic complexity that captures the non-linear\nrelationship with the accompanying music.\nIn this work, we address these challenges by presenting\na novel Full Attention Cross-modal Transformer (FACT)\nnetwork, which can robustly generate realistic 3D dance\nmotion from music, along with a large-scale multi-modal\n3D dance motion dataset, AIST++, to train such a model.\nSpeci\ufb01cally, given a piece of music and a short (2 seconds)\nseed motion, our model is able to generate a long sequence\nof realistic 3D dance motions. Our model effectively learns\nthe music-motion correlation and can generate dance se-\n1\narXiv:2101.08779v3  [cs.CV]  31 Jul 2021\n\n\nFigure 2: Cross-Modal Music Conditioned 3D Motion Generation Overview. Our proposed a Full-Attention Cross-\nmodal Transformer (FACT) network (details in Figure 3) takes in a music piece and a 2-second sequence of seed motion,\nthen auto-regressively generates long-range future motions that correlates with the input music.\nquences that varies for different input music. We represent\ndance as a 3D motion sequence that consists of joint rota-\ntion and global translation, which enables easy transfer of\nour output for applications such as motion retargeting as\nshown in Figure 1.\nIn order to generate 3D dance motion from music, we\npropose a novel Full Attention Cross-modal Transformer\n(FACT) model, which employs an audio transformer and\nseed motion transformer to encode the inputs, which are\nthen fused by a cross-modal transformer that models the dis-\ntribution between audio and motion. This model is trained\nto predict N future motion sequences and at test time is ap-\nplied in an auto-regressive manner to generate continuous\nmotion. The success of our model relies on three key design\nchoices: 1) the use of full-attention in an auto-regressive\nmodel, 2) future-N supervision, and 3) early fusion of two\nmodalities. The combination of these choices is critical for\ntraining a model that can generate a long realistic dance mo-\ntion that is attuned to the music. Although prior work has\nexplored using transformers for motion generation [3], we\n\ufb01nd that naively applying transformers to the 3D dance gen-\neration problem without these key choices does not lead to\na very effective model.\nIn particular, we notice that because the context window\nin the motion domain is signi\ufb01cantly smaller than that of\nlanguage models, it is possible to apply full-attention trans-\nformers in an auto-regressive manner, which leads to a more\npowerful model.\nIt is also critical that the full-attention\ntransformer is trained to predict N possible future motions\ninstead of one. These two design choices are key for pre-\nventing 3D motion from freezing or drifting after several\nauto-regressive steps as reported in prior works on 3D mo-\ntion generation [4, 3]. Our model is trained to predict 20\nfuture frames, but it is able to produce realistic 3D dance\nmotion for over 1200 frames at test time. We also show that\nfusing the two modalities early, resulting in a deep cross-\nmodal transformer, is important for training a model that\ngenerates different dance sequences for different music.\nIn order to train the proposed model, we also address\nthe problem of data. While there are a few motion capture\ndatasets of dancers dancing to music, collecting mocap data\nrequires heavily instrumented environments making these\ndatasets severely limited in the number of available dance\nsequences, dancer and music diversity. In this work, we pro-\npose a new dataset called AIST++, which we build from the\nexisting multi-view dance video database called AIST [82].\nWe use the multi-view videos to recover reliable 3D motion\nfrom this data. We will release code and this dataset for re-\nsearch purposes, where AIST++ can be a new benchmark\nfor the task of 3D dance generation conditioned on music.\nIn summary, our contributions are as follows:\n\u2022 We propose Full Attention Cross-Modal Transformer\nmodel, FACT, which can generate a long sequence of\nrealistic 3D dance motion that is well correlated with\nthe input music.\n\u2022 We introduce AIST++ dataset containing 5.2 hours of\n3D dance motions accompanied with music and multi-\nview images, which to our knowledge is the largest\ndataset of such kind.\n\u2022 We provide extensive evaluations validating our design\nchoices and show that they are critical for high quality,\nmulti-modal, long motion sequence generation.",
      "type": "introduction",
      "word_count": 831
    },
    {
      "title": "2. Related Work",
      "content": "3D Human Motion Synthesis\nThe problem of generating\nrealistic and controllable 3D human motion sequences has\nlong been studied. Earlier works employ statistical models\nsuch as kernel-based probability distribution [64, 10, 25,\n11] to synthesize motion, but abstract away motion details.\nMotion graphs [53, 7, 47] address this problem by generat-\ning motions in a non-parametric manner. Motion graph is\na directed graph constructed on a corpus of motion capture\ndata, where each node is a pose and the edges represent the\ntransition between poses. Motion is generated by a random\nwalk on this graph. A challenge in motion graph is in gener-\nating plausible transition that some approaches address via\nparameterizing the transition [30]. With the development\nin deep learning, many approaches explore the applicabil-\nity of neural networks to generate 3D motion by training on\na large-scale motion capture dataset, where network archi-\ntectures such as CNNs [35, 34], GANs [31], RBMs [80],\n\n\nRNNs [24, 4, 40, 27, 16, 18, 88, 12, 87] and Transform-\ners [3, 9] have been explored. Auto-regressive models like\nRNNs and vanilla Transformers are capable of generating\nunbounded motion in theory, but in practice suffer from\nregression to the mean where motion \u201cfreezes\u201d after sev-\neral iterations, or drift to unnatural motions [4, 3]. Some\nworks [8, 56, 49] propose to ease this problem by peri-\nodically using the network\u2019s own outputs as inputs during\ntraining. Phase-functioned neural networks and it\u2019s varia-\ntions [94, 33, 73, 74] address this issue via conditioning the\nnetwork weights on phase, however, they do not scale well\nto represent a wide variety of motion.\nAudio To Human Motion Generation\nAudio to motion\ngeneration has been studied in 2D pose context either in\noptimization based approach [81], or learning based ap-\nproaches [52, 72, 51, 67, 68, 21] where 2D pose skele-\ntons are generated from a conditioning audio. Training data\nfor 2D pose and audio is abundant thanks to the high re-\nliability of 2D pose detectors [14]. However, predicting\nmotion in 2D is limited in its expressiveness and poten-\ntial for downstream applications.\nFor 3D dance genera-\ntion, earlier approaches explore matching existing 3D mo-\ntion to music [71] using motion graph based approach [20].\nMore recent approach employ LSTMs [5, 79, 90, 97, 42],\nGANs [51, 78, 28], transformer encoder with RNN de-\ncoder [36] or convolutional [2, 92] sequence-to-sequence\nmodels.\nConcurrent to our work, Chen et al. [15] pro-\nposed a method that is based on motion graphs with learned\nembedding space. Many prior works [72, 68, 42, 28, 92]\nsolve this problem by predicting future motion determinis-\ntically from audio without seed motion. When the same au-\ndio has multiple corresponding motions, which often occurs\nin dance data, these methods collapse to predicting a mean\npose. In contrast, we formulate the problem with seed mo-\ntion as in [55, 96], which allows generation of multiple mo-\ntion from the same audio even with a deterministic model.\nClosest to our work is that of Li et al. [55], which also\nemploy transformer based architecture but only on audio\nand motion. Furthermore, their approach discretize the out-\nput joint space in order to account for multi-modality, which\ngenerates unrealistic motion. In this work we introduce a\nnovel full-attention based cross-modal transformer (FACT\nmodel) for audio and motion, which can not only preserve\nthe correlation between music and 3D motion better, but\nalso generate more realistic long 3D human motion with\nglobal translation.\nOne of the biggest bottleneck in 3D\ndance generation approaches is that of data. Recent work\nof Li et al. [55] reconstruct 3D motion from dance videos\non the Internet, however the data is not public. Further, us-\ning 3D motion reconstructed from monocular videos may\nnot be reliable and lack accurate global 3D translation in-\nformation. In this work we also reconstruct the 3D motion\nfrom 2D dance video, but from multi-view video sequences,\nwhich addresses these issues. While there are many large\nscale 3D motion capture datasets [39, 59, 1, 37], mocap\ndataset of 3D dance is quite limited as it requires heavy\ninstrumentation and expert dancers for capture. As such,\nmany of these previous works operate on either small-scale\nor private motion capture datasets [79, 5, 96]. We compare\nour proposed dataset with these public datasets in Table 1.\nCross-Modal Sequence-to-Sequence Generation\nBe-\nyond of the scope of human motion generation, our work\nis closely related to the research of using neural network\non cross-modal sequence to sequence generation task. In\nnatural language processing and computer vision, tasks\nlike text to speech (TTS) [69, 41, 43, 83] and speech\nto gesture [22, 28, 23], image/video captioning (pixels to\ntext) [13, 44, 58, 48] involve solving the cross-modal se-\nquence to sequence generation problem. Initially, combina-\ntion of CNNs and RNNs [86, 85, 91, 93] were prominent in\napproaching this problem. More recently, with the develop-\nment of attention mechanism [84], transformer based net-\nworks achieve top performance for visual-text [95, 77, 19,\n54, 38, 76, 76], visual-audio [26, 89] cross-modal sequence\nto sequence generation task. Our work explores audio to\n3D motion in a transformer based architecture. While all\ncross-modal problems induce its own challenges, the prob-\nlem of music to 3D dance is uniquely challenging in that\nthere are many ways to dance to the same music and that the\nsame dance choreography may be used for multiple music.\nWe hope the proposed AIST++ dataset advances research in\nthis relatively under-explored problem.\n3. AIST++ Dataset\nData Collection We generate the proposed 3D motion\ndataset from an existing database called AIST Dance\nDatabase [82]. AIST is only a collection of videos with-\nout any 3D information. Although it contains multi-view\nvideos of dancers, these cameras are not calibrated, making\n3D reconstruction of dancers a non-trivial effort. We re-\ncover the camera calibration parameters and the 3D human\nmotion in terms of SMPL parameters. Please \ufb01nd the details\nof this algorithm in the Appendix. Although we adopt the\nbest practices in reconstructing this data, no code base exist\nfor this particular problem setup and running this pipeline\non a large-scale video dataset requires non-trivial amount of\ncompute and effort. We will make the 3D data and camera\nparameters publicly available, which allows the community\nto benchmark on this dataset on an equal footing.\nDataset Description\nResulting AIST++ is a large-scale\n3D human dance motion dataset that contains a wide variety\nof 3D motion paired with music. It has the following extra\nannotations for each frame:\n\u2022 9 views of camera intrinsic and extrinsic parameters;\n\n\nDataset\nMusic\n3D Jointpos\n3D Jointrot\n2D Kpt\nViews\nImages\nGenres\nSubjects\nSequences\nSeconds\nAMASS[59]\n\u0017\n\u0013\n\u0013\n\u0017\n0\n0\n0\n344\n11265\n145251\nHuman3.6M[39]\n\u0017\n\u0013\n\u0013\n\u0013\n4\n3.6M\n0\n11\n210\n71561\nDance with Melody[79]\n\u0013\n\u0013\n\u0017\n\u0017\n0\n0\n4\n-\n61\n5640\nGrooveNet [5]\n\u0013\n\u0013\n\u0017\n\u0017\n0\n0\n1\n1\n2\n1380\nDanceNet [96]\n\u0013\n\u0013\n\u0017\n\u0017\n0\n0\n2\n2\n2\n3472\nEA-MUD [78]\n\u0013\n\u0013\n\u0017\n\u0017\n0\n0\n4\n-\n17\n1254\nAIST++\n\u0013\n\u0013\n\u0013\n\u0013\n9\n10.1M\n10\n30\n1408\n18694\nTable 1: 3D Dance Datasets Comparisons. The proposed AIST++ dataset is the largest dataset with 3D dance motion paired\nwith music. We also have the largest variety of subjects and genres. Furthermore, our dataset is the only one that comes with\nimage frames, as other dance datasets only contain motion capture dataset. We include popular 3D motion dataset without\nany music in the \ufb01rst two rows for reference.\n\u2022 17 COCO-format[70] human joint locations in both\n2D and 3D;\n\u2022 24 SMPL [57] pose parameters along with the global\nscaling and translation.\nBesides the above properties, AIST++ dataset also contains\nmulti-view synchronized image data unlike prior 3D dance\ndataset, making it useful for other research directions such\nas 2D/3D pose estimation. To our knowledge, AIST++ is\nthe largest 3D human dance dataset with 1408 sequences,\n30 subjects and 10 dance genres with basic and advanced\nchoreographies. See Table. 1 for comparison with other 3D\nmotion and dance datasets. AIST++ is a complementary\ndataset to existing 3D motion dataset such as AMASS [59],\nwhich contains only 17.8 minutes of dance motions with no\naccompanying music.\nOwing to the richness of AIST, AIST++ contains 10\ndance genres: Old School (Break, Pop, Lock and Waack)\nand New School (Middle Hip-hop, LA-style Hip-hop,\nHouse, Krump, Street Jazz and Ballet Jazz).\nPlease see\nthe Appendix for more details and statistics. The motions\nare equally distributed among all dance genres, covering\nwide variety of music tempos denoted as beat per minute\n(BPM)[61]. Each genre of dance motions contains 85%\nof basic choreographies and 15% of advanced choreogra-\nphies, in which the former ones are those basic short danc-\ning movements while the latter ones are longer movements\nfreely designed by the dancers. However, note that AIST is\nan instructional database and records multiple dancers danc-\ning the same choreography for different music with varying\nBPM, a common practice in dance. This posits a unique\nchallenge in cross-modal sequence-to-sequence generation.\nWe carefully construct non-overlapping train and val sub-\nsets on AIST++ to make sure neither choreography nor mu-\nsic is shared across the subsets.\n4. Music Conditioned 3D Dance Generation\nHere we describe our approach towards the problem of\nmusic conditioned 3D dance generation. Speci\ufb01cally, given\na 2-second seed sample of motion represented as X =\n(x1, . . . , xT ) and a longer conditioning music sequence\nrepresented as Y = (y1, . . . , yT \u2032), the problem is to gen-\nerate a sequence of future motion X\u2032 = (xT +1, . . . , xT \u2032)\nfrom time step T + 1 to T \u2032, where T \u2032 \u226bT.\nPreliminaries\nTransformer [84] is an attention based net-\nwork widely applied in natural language processing. A ba-\nsic transformer building block (shown in of Figure 3 (a)) has\nmultiple layers with each layer composed of a multi-head\nattention-layer (Attn) followed by a feed forward layer (FF).\nThe multi-head attention-layer embeds input sequence X\ninto an internal representation often referred to as the con-\ntext vector C. Speci\ufb01cally, the output of the attention layer,\nthe context vector C is computed using the query vector Q\nand the key K value V pair from input with or without a\nmask M via,\nC = FF(Attn(Q, K, V, M))\n= FF(softmax\n \nQKT + M\n\u221a\nD\n!\nV),\nQ = XWQ, K = XWK, V = XWV\n(1)\nwhere D is the number of channels in the attention layer\nand W are trainable weights. The design of the mask func-\ntion is a key parameter in a transformer. In natural language\ngeneration, causal models such as GPT [66] uses an upper\ntriangular look-ahead mask M to enable causal attention\nwhere each token can only look at past inputs. This allows\nef\ufb01cient inference at test time, since intermediate context\nvectors do not need to be recomputed, especially given the\nlarge context window in these models (2048). On the other\nhand, models like BERT [17] employ full-attention for fea-\nture learning, but rarely are these models employed in an\nauto-regressive manner, due to its inef\ufb01ciency at test time.\n4.1. Full Attention Cross-Modal Transformer\nWe propose Full Attention Cross-Modal Transformer\n(FACT) model for the task of 3D dance motion generation.\nGiven the seed motion X and audio features Y, FACT \ufb01rst\nencodes these inputs using a motion transformer fmot and\naudio transformer faudio into motion and audio embeddings\nhx1:T and hy1:T \u2032 respectively. These are then concatenated\n\n\nTransformer Network\nFigure 3: FACT Model Details. (a) The structure of the audio/motion/cross-modal transformer with N attention layers. (b)\nAttention and supervision mechanism as a simpli\ufb01ed two-layer model. Models like GPT [66] and the motion generator of [55]\nuse causal attention (left) to predict the immediate next output for each input nodes. We employ full-attention and predict n\nfuture from the last input timestamp m (right). The dots on the bottom row are the input tensors, which are computed into\ncontext tensors through causal (left) and full (right) attention transformer layer. The output (predictions) are shown on the\ntop. We empirically show that these design choices are critical in generating non-freezing, more realistic motion sequences.\nand sent to a cross-modal transformer fcross, which learns\nthe correspondence between both modalities and generates\nN future motion sequences X\u2032, which is used to train the\nmodel in a self-supervised manner. All three transformers\nare jointly learned in an end-to-end manner. This process is\nillustrated in Figure 2. At test time, we apply this model in\nan auto-regressive framework, where we take the \ufb01rst pre-\ndicted motion as the input of the next generation step and\nshift all conditioning by one.\nFACT involves three key design choices that are critical\nfor producing realistic 3D dance motion from music. First,\nall of the transformers use full-attention mask. We can still\napply this model ef\ufb01ciently in an auto-regressive framework\nat test time, since our context window is not prohibitively\nlarge (240). The full-attention model is more expressive\nthan the causal model because internal tokens have access\nto all inputs. Due to this full-attention design, we train our\nmodel to only predict the unseen future after the context\nwindow. In particular, we train our model to predict N fu-\ntures beyond the current input instead of just 1 future mo-\ntion. This encourages the network to pay more attention to\nthe temporal context, and we experimentally validate that\nthis is a key factor training a model that does not suffer\nfrom motion freezing or diverging after a few generation\nsteps. This attention design is in contrast to prior work that\nemploy transformers for the task of 3D motion [3] or dance\ngeneration [55], which applies GPT [66] style causal trans-\nformer trained to predict the immediate next future token.\nWe illustrate this difference in Figure 3 (b).\nLastly, we fuse the two embeddings early and employ a\ndeep 12-layer cross-modal transformer module. This is in\ncontrast to prior work that used a single MLP to combine\nthe audio and motion embeddings [55], and we \ufb01nd that\ndeep cross-modal module is essential for training a model\nthat actually pays attention to the input music. This is par-\nticularly important as in dance, similar choreography can be\nused for multiple music. This also happens in AIST dataset,\nand we \ufb01nd that without a deep cross-modal module, the\nnetwork is prone to ignoring the conditioning music. We\nexperimentally validate this in Section 5.2.3.",
      "type": "related_work",
      "word_count": 2420
    },
    {
      "title": "5. Experiments",
      "content": "5.1. AIST++ Motion Quality Validation\nWe \ufb01rst carefully validate the quality of our 3D motion\nreconstruction. Possible error sources that may affect the\nquality of our 3D reconstruction include inaccurate 2D key-\npoints detection and the estimated camera parameters. As\nthere is no 3D ground-truth for AIST dataset, our validation\nhere is based-on the observation that the re-projected 2D\nkeypoints should be consistent with the predicted 2D key-\npoints which have high prediction con\ufb01dence in each im-\nage. We use the 2D mean per joint position error MPJPE-\n2D, commonly used for 3D reconstruction quality measure-\nment [46, 39, 65]) to evaluate the consistency between the\npredicted 2D keypoints and the reconstructed 3D keypoints\nalong with the estimated camera parameters. Note we only\nconsider 2D keypoints with prediction con\ufb01dence over 0.5\nto avoid noise. The MPJPE-2D of our entire dataset is 6.2\npixels on the 1920\u00d71080 image resolution, and over 86% of\nthose has less than 10 pixels of error. Besides, we also cal-\nculate the PCKh metric introduced in [6] on our AIST++.\nThe PCKh@0.5 on the whole set is 98.7%, meaning the\nreconstructed 3D keypoints are highly consistent with the\npredicted 2D keypoints. Please refer to the Appendix for\ndetailed analysis of MPJPE-2D and PCKh on AIST++.\n5.2. Music Conditioned 3D Motion Generation\n5.2.1",
      "type": "experiments",
      "word_count": 216
    },
    {
      "title": "Experimental Setup",
      "content": "Dataset Split\nAll the experiments in this paper are con-\nducted on our AIST++ dataset, which to our knowledge is\nthe largest dataset of this kind. We split AIST++ into train\nand test set, and report the performance on the test set only.\nWe carefully split the dataset to make sure that the music\nand dance motion in the test set does not overlap with that\nin the train set. To build the test set, we \ufb01rst select one mu-\nsic piece from each of the 10 genres. Then for each music\n\n\nMotion Quality\nMotion Diversity\nMotion-Music Corr\nUser Study\nFIDk \u2193\nFIDg \u2193\nDistk \u2191\nDistg \u2191\nBeatAlign \u2191\nFACT WinRate\u2193\nAIST++\n\u2013\n\u2013\n9.057\n7.556\n0.292\n\u2013\nAIST++ (random)\n\u2013\n\u2013\n\u2013\n\u2013\n0.213\n25.4%\nLi et al. [55]\n86.43\n20.58\n6.85*\n4.93\n0.232\n80.6%\nDancenet [96]\n69.18\n17.76\n2.86\n2.72\n0.232\n71.1%\nDanceRevolution [36]\n73.42\n31.01\n3.52\n2.46\n0.220\n77.0%\nFACT (ours)\n35.35\n12.40\n5.94\n5.30\n0.241\n\u2013\nTable 2: Conditional Motion Generation Evaluation on AIST++ dataset. Comparing to the three recent state-of-the-art\nmethods, our model generates motions that are more realistic, better correlated with input music and more diversi\ufb01ed when\nconditioned on different music. *Note Li et al. [55]\u2019s generated motions are discontinuous making its average kinetic feature\ndistance (FIDk) abnormally high.\npiece, we randomly select two dancers, each with two dif-\nferent choreographies paired with that music, resulting in\ntotal 40 unique choreographies in the test set. The train set\nis built by excluding all test musics and test choreographies\nfrom AIST++, resulting in total 329 unique choreographies\nin the train set. Note that in the test set we intentionally\npick music pieces with different BPMs so that it covers all\nkinds of BPMs ranging from 80 to 135 in AIST++.\nImplementation Details\nIn our main experiment, the in-\nput of the model contains a seed motion sequence with 120\nframes (2 seconds) and a music sequence with 240 frames\n(4 seconds), where the two sequences are aligned on the\n\ufb01rst frame. The output of the model is the future motion se-\nquence with N = 20 frames supervised by L2 loss. During\ninference we continually generate future motions in a auto-\nregressive manner at 60 FPS, where only the \ufb01rst predicted\nmotion is kept in every step. We use the publicly available\naudio processing toolbox Librosa [60] to extract the music\nfeatures including: 1-dim envelope, 20-dim MFCC, 12-dim\nchroma, 1-dim one-hot peaks and 1-dim one-hot beats, re-\nsulting in a 35-dim music feature. We combine the 9-dim\nrotation matrix representation for all 24 joints, along with a\n3-dim global translation vector, resulting in a 219-dim mo-\ntion feature. Both these raw audio and motion features are\n\ufb01rst embedded into 800-dim hidden representations with\nlinear layers, then added with learnable positional encod-\ning, before they were input into the transformer layers. All\nthe three (audio, motion, cross-modal) transformers have 10\nattention heads with 800 hidden size. The number of atten-\ntion layers in each transformer varies based on the exper-\niments, as described in Sec. 5.2.3. We disregard the last\nlinear layer in the audio/motion transformer and the posi-\ntional encoding in the cross-modal transformer, as they are\nnot necessary in the FACT model. All our experiments are\ntrained with 16 batch size using Adam [45] optimizer. The\nlearning rate starts from 1e\u22124 and drops to {1e\u22125, 1e\u22126}\nafter {60k, 100k} steps. The training \ufb01nishes after 300k,\nwhich takes 3 days on 4 TPUs. For baselines, we compare\nwith the latest work on 3D dance generation that take mu-\nsic and seed motion as input, including Dancenet [96] and\nLi et al. [55]. For a more comprehensive evaluation we also\ncompare with the recent state-of-the-art 2D dance genera-\ntion method DanceRevolution [36]. We adapt this work to\noutput 3D joint locations which can be directly compared\nwith our results quantitatively, though joint locations do not\nallow immediate re-targeting. We train and test these base-\nlines on the same dataset with ours using the of\ufb01cial code\nprovided by the authors.\n5.2.2\nQuantitative Evaluation\nIn this section, we evaluate our proposed model FACT on\nthe following aspects: (1) motion quality, (2) generation di-\nversity and (3) motion-music correlation. Experiments re-\nsults (shown in Table 2) show that our model out-performs\nstate-of-the-art methods [55, 36, 96], on those criteria.\nMotion Quality\nSimilar to prior works [55, 36], we eval-\nuate the generated motion quality by calculating the distri-\nbution distance between the generated and the ground-truth\nmotions using Frechet Inception Distance (FID) [32] on\nthe extracted motion features. As prior work used motion-\nencoders that are not public, we measure FID with two well-\ndesigned motion feature extractors [62, 63] implemented in\nfairmotion [29]: (1) a geometric feature extractor that pro-\nduces a boolean vector zg \u2208R33 expressing geometric re-\nlations between certain body points in the motion sequence\nX \u2208RT \u00d7N\u00d73, (2) a kinetic feature extractor [63] that maps\na motion sequence X to zk \u2208R72, which represents the\nkinetic aspects of the motion such as velocity and acceler-\nations. We denote the FID based on these geometric and\nkinetic features as FIDgand FIDk, respectively. The metrics\nare calculated between the real dance motion sequences in\nAIST++ test set and 40 generated motion sequences each\nwith T = 1200 frames (20 secs). As shown in Table 2, our\ngenerated motion sequences have a much closer distribution\nto ground-truth motions compared with the three baselines.\nWe also visualize the generated sequences from the base-\nlines in our supplemental video.\n\n\nGenerated 3D Dance Motion\nSeed Motion\nFigure 4: Diverse Generation Results. Here we visualize\n4 different dance motions generated using different music\nbut the same seed motion. On the left we illustrate the 2\nsecond seed motion and on the right we show the gener-\nated 3D dance sequences subsampled by 2 seconds. For\nrows top to bottom, the genres of the conditioning music\nare: Break, Ballet Jazz, Krump and Middle Hip-hop. Note\nthat the seed motion come from hip-hop dance.Our model\nis able to adapt the dance style when given a more modern\ndance music (second row: Ballet Jazz). Please see more\nresults in the supplementary video.\nGeneration Diversity\nWe also evaluate our model\u2019s abil-\nity to generate diverse dance motions when given various\ninput music compared with the baseline methods. Similar\nto the prior work [36], we calculate the average Euclidean\ndistance in the feature space across 40 generated motions on\nthe AIST++ test set to measure the diversity. The motion\ndiversity in the geometric feature space and in the kinetic\nfeature space are noted as Distmand Distk, respectively. Ta-\nble 2 shows that our method generates more diverse dance\nmotions comparing to the baselines except Li et al. [55],\nwhich discretizes the motion, leading to discontinuous out-\nputs that results in high Distk. Our generated diverse mo-\ntions are visualized in Figure 4.\nMotion-Music Correlation\nFurther, we evaluate how\nmuch the generated 3D motion correlates to the input mu-\nsic. As there is no well-designed metric to measure this\nproperty, we propose a novel metric, Beat Alignment Score\n(BeatAlign), to evaluate the motion-music correlation in\nterms of the similarity between the kinematic beats and mu-\nsic beats. The music beats are extracted using librosa [60]\nand the kinematic beats are computed as the local minima of\nthe kinetic velocity, as shown in Figure 5. The Beat Align-\nment Score is then de\ufb01ned as the average distance between\nevery kinematic beat and its nearest music beat. Speci\ufb01-\ncally, our Beat Alignment Score is de\ufb01ned as:\nBeatAlign = 1\nm\nm\nX\ni=1\nexp(\u2212\nmin\u2200ty\nj \u2208By ||tx\ni \u2212ty\nj||2\n2\u03c32\n)\n(2)\nwhere Bx = {tx\ni } is the kinematic beats, By = {ty\nj} is the\nmusic beats and \u03c3 is a parameter to normalize sequences\ntime\nkinetic velocity\nmusic beats\nkinematic beats\nFigure 5: Beats Alignment between Music and Gener-\nated Dance. Here we visualize the kinetic velocity (blue\ncurve) and kinematic beats (green dotted line) of our gener-\nated dance motion, as well as the music beats (orange dot-\nted line). The kinematic beats are extracted by \ufb01nding local\nminima from the kinetic velocity curve.\nwith different FPS. We set \u03c3 = 3 in all our experiments\nas the FPS of all our experiments sequences is 60. A sim-\nilar metric Beat Hit Rate was introduced in [51, 36], but\nthis metric requires a dataset dependent handcrafted thresh-\nold to decide the alignment (\u201chit\u201d) while ours directly mea-\nsure the distances. This metric is explicitly designed to be\nuni-directional as dance motion does not necessarily have\nto match with every music beat. On the other hand, every\nkinetic beat is expected to have a corresponding music beat.\nTo calibrate the results, we compute the correlation met-\nrics on the entire AIST++ dataset (upper bound) and on the\nrandom-paired data (lower bound). As shown in Table 2,\nour generated motion is better correlated with the input mu-\nsic compared to the baselines. We also show one example\nin Figure 5 that the kinematic beats of our generated motion\nalign well with the music beats. However, when comparing\nto the real data, all four methods including ours have a large\nspace for improvement. This re\ufb02ects that music-motion cor-\nrelation is still a challenging problem.\n5.2.3\nAblation Study\nWe conduct the following ablation experiments to study\nthe effectiveness of our key design choices: Full-Attention\nFuture-N supervision, and early cross-modal fusion. Please\nrefer to our supplemental video for qualitative comparison.\nThe effectiveness of different model architectures is mea-\nsured quantitatively using the motion quality (FIDk, FIDg)\nand the music-motion correlation (BeatAlign) metrics, as\nshown in Table 4 and Table 3.\nFull-Attention Future-N Supervision\nHere we dive\ndeep into the attention mechanism and our future-N super-\nvision scheme. We set up four different settings: causal-\nattention shift-by-1 supervision, and full-attention with\nfuture-{1, 10, 20} supervision. Qualitatively, we \ufb01nd that\nthe motion generated by the causal-attention with shift-by-\n1 supervision (as done in [55, 66, 3]) starts to freeze after\nseveral seconds (please see the supplemental video). Sim-\nilar problem was reported in the results of [3]. Quantita-\ntively (shown in the Table 3), when using causal-attention\nshift-by-1 supervision, the FIDs are large meaning that the\ndifference between generated and ground-truth motion se-\n\n\nAttn-Supervision\nFIDk \u2193\nFIDg \u2193\nBeatAlign \u2191\nCausal-Attn-Shift-by-1\n111.69\n21.43\n0.217\nFull-Attn-F1 (FACT-1)\n207.74\n19.35\n0.233\nFull-Attn-F10 (FACT-10)\n35.10\n15.17\n0.239\nFull-Attn-F20 (FACT-20)\n35.35\n12.39\n0.241\nTable 3: Ablation Study on Attention and Supervision\nMechanism. Causal-attention shift-by-1 supervision tends\nto generate freezing motions in the long-term. While Full-\nattention supervised more future frames boost the ability of\ngenerating more realistic dance motions.\nmotion attention\nmusic attention\n(a)\n(b)\nlow\nhigh\nFigure 6: Attention Weights Visualization. We compare\nthe attention weights from the last layer of the (a) 12-layer\ncross-modal transformer and (b) 1-layer cross-modal trans-\nformer. Deeper cross-modal transformer pays equal atten-\ntion to motion and music, while a shallower one pays more\nattention to motion.\nquences is substantial. For the full-attention with future-\n1 supervision setting, the results rapidly drift during long-\nrange generation. However, when the model is supervised\nwith 10 or 20 future frames, it pays more attention to the\ntemporal context. Thus, it learns to generate good quality\n(non-freezing, non-drifting) long-range motion.\nEarly Cross-Modal Fusion\nHere we investigate when to\nfuse the two input modalities. We conduct experiments in\nthree settings, (1) No-Fusion: 14-layer motion transformer\nonly; (2) Late-Fusion: 13-layer motion/audio transformer\nwith 1-layer cross-modal transformer; (3) Early-Fusion: 2-\nlayer motion/audio transformer with 12-layer cross-modal\ntransformer. For fair comparison, we change the number\nof attention layers in the motion/audio transformer and the\ncross-modal transformer but keep the total number of the\nattention layers \ufb01xed. Table 4 shows that the early fusion\nbetween two input modalities is critical to generate motions\nthat are well correlated with input music. Also we show\nin Figure 6 that Early-Fusion allows the cross-model trans-\nformer pays more attention to the music, while Late-Fusion\ntend to ignore the conditioning music.\nThis also aligns\nwith our intuition that the two modalities need to be fully\nfused for better cross-modal learning, as contrast to prior\nwork that uses a single MLP to combine the audio and mo-\ntion [55].\n5.2.4\nUser Study\nFinally, we perceptually evaluate the motion-music corre-\nlation with a user study to compare our method with the\nthree baseline methods and the \u201crandom\u201d baseline, which\nrandomly combines AIST++ motion-music. (Refer to the\nAppendix for user study details.) In this study, each user\nCross-Modal Fusion\nFIDk \u2193\nFIDg \u2193\nBeatAlign \u2191\nNo-Fusion\n45.66\n13.27\n0.228*\nLate-Fusion\n45.76\n14.30\n0.234\nEarly-Fusion\n35.35\n12.39\n0.241\nTable 4: Ablation Study on Cross-modal Fusion. Early\nfusion of the two modalities allows the model to generate\nmotion sequences align better with the conditioning music.\n*Note this number is calculated using the music paired with\nthe input motion.\nis asked to watch 10 videos showing one of our results and\none random counterpart, and answer the question \u201cwhich\nperson is dancing more to the music? LEFT or RIGHT\u201d for\neach video. For user study on each of the four baselines, we\ninvite 30 participants, ranging from professional dancers to\npeople who rarely dance. We analyze the feedback and the\nresults are: (1) 81% of our generated dance motion is better\nthan Li et al. [55]; (2) 71% of our generated dance motion is\nbetter than Dancenet [96]; (3) 77% of our generated dance\nmotion is better than DanceRevolution [36]; (4) 75% of the\nunpaired AIST++ dance motion is better than ours. Clearly\nwe surpass the baselines in the user study. But because the\n\u201crandom\u201d baseline consists of real advanced dance motions\nthat are extremely expressive, participants are biased to pre-\nfer it over ours. However, quantitative metrics show that our\ngenerated dance is more aligned with music.\n6. Conclusion and Discussion\nIn this paper, we present a cross-modal transformer-\nbased neural network architecture that can not only learn\nthe audio-motion correspondence but also can generate non-\nfreezing high quality 3D motion sequences conditioned on\nmusic.\nWe also construct the largest 3D human dance\ndataset: AIST++. This proposed, multi-view, multi-genre,\ncross-modal 3D motion dataset can not only help research\nin the conditional 3D motion generation research but also\nhuman understanding research in general. While our results\nshows a promising direction in this problem of music condi-\ntioned 3D motion generation, there are more to be explored.\nFirst, our approach is kinematic based and we do not reason\nabout physical interactions between the dancer and the \ufb02oor.\nTherefore the global translation can lead to artifacts such as\nfoot sliding and \ufb02oating. Second, our model is currently\ndeterministic. Exploring how to generate multiple realistic\ndance per music is an exciting direction.\n7. Acknowledgement\nWe thank Chen Sun, Austin Myers, Bryan Seybold and\nAbhijit Kundu for helpful discussions. We thank Emre Ak-\nsan and Jiaman Li for sharing their code. We also thank\nKevin Murphy for the early attempts on this direction, as\nwell as Peggy Chi and Pan Chen for the help on user study\nexperiments.",
      "type": "experiments",
      "word_count": 2510
    },
    {
      "title": "References",
      "content": "[1] Mixamo. https://www.mixamo.com/. 1, 3\n[2] Hyemin Ahn, Jaehun Kim, Kihyun Kim, and Songhwai Oh.\nGenerative autoregressive networks for 3d dancing move\nsynthesis from music. IEEE Robotics and Automation Let-\nters, 5(2):3500\u20133507, 2020. 3\n[3] Emre Aksan, Peng Cao, Manuel Kaufmann, and Otmar\nHilliges.\nAttention, please:\nA spatio-temporal trans-\nformer for 3d human motion prediction.\narXiv preprint\narXiv:2004.08692, 2020. 2, 3, 5, 7\n[4] Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc-\ntured prediction helps 3d human motion modelling. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 7144\u20137153, 2019. 2, 3\n[5] Omid Alemi,\nJules Franc\u00b8oise,\nand Philippe Pasquier.\nGroovenet: Real-time music-driven dance movement gen-\neration using arti\ufb01cial neural networks. networks, 8(17):26,\n2017. 3, 4\n[6] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2d human pose estimation: New benchmark\nand state of the art analysis. In Proceedings of the IEEE Con-\nference on computer Vision and Pattern Recognition, pages\n3686\u20133693, 2014. 5\n[7] Okan Arikan and David A Forsyth. Interactive motion gen-\neration from examples.\nACM Transactions on Graphics\n(TOG), 21(3):483\u2013490, 2002. 2\n[8] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer. Scheduled sampling for sequence prediction with\nrecurrent neural networks. In Advances in neural informa-\ntion processing systems, 2015. 3\n[9] Uttaran Bhattacharya,\nNicholas Rewkowski,\nAbhishek\nBanerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha.\nText2gestures: A transformer-based network for generating\nemotive body gestures for virtual agents.\narXiv preprint\narXiv:2101.11101, 2021. 3\n[10] Richard Bowden. Learning statistical models of human mo-\ntion. In IEEE Workshop on Human Modeling, Analysis and\nSynthesis, CVPR, volume 2000, 2000. 2\n[11] Matthew Brand and Aaron Hertzmann. Style machines. In\nProceedings of the 27th annual conference on Computer\ngraphics and interactive techniques, pages 183\u2013192, 2000.\n2\n[12] Judith B\u00a8utepage, Michael J Black, Danica Kragic, and Hed-\nvig Kjellstr\u00a8om. Deep representation learning for human mo-\ntion prediction and classi\ufb01cation. In CVPR, page 2017, 2017.\n3\n[13] Slides by Saheel. Baby talk: Understanding and generating\nimage descriptions. 3\n[14] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\nYaser Sheikh.\nOpenPose: realtime multi-person 2D pose\nestimation using Part Af\ufb01nity Fields.\nIn arXiv preprint\narXiv:1812.08008, 2018. 3\n[15] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-\nChen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster:\nchoreography-oriented music-driven dance synthesis. ACM\nTransactions on Graphics (TOG), 40(4):1\u201313, 2021. 3\n[16] Hsu-kuang Chiu, Ehsan Adeli, Borui Wang, De-An Huang,\nand Juan Carlos Niebles. Action-agnostic human pose fore-\ncasting. In 2019 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 1423\u20131432. IEEE, 2019. 3\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 4\n[18] Xiaoxiao Du, Ram Vasudevan, and Matthew Johnson-\nRoberson.\nBio-lstm: A biomechanically inspired recur-\nrent neural network for 3-d pedestrian pose and gait pre-\ndiction. IEEE Robotics and Automation Letters, 4(2):1501\u2013\n1508, 2019. 3\n[19] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong\nWang, Wenwu Zhu, and Junzhou Huang. Weakly supervised\ndense event captioning in videos. In Advances in Neural In-\nformation Processing Systems, pages 3059\u20133069, 2018. 3\n[20] Rukun Fan, Songhua Xu, and Weidong Geng.\nExample-\nbased automatic music-driven conventional dance motion\nsynthesis. IEEE transactions on visualization and computer\ngraphics, 18(3):501\u2013515, 2011. 3\n[21] Joao P Ferreira, Thiago M Coutinho, Thiago L Gomes,\nJos\u00b4e F Neto, Rafael Azevedo, Renato Martins, and Erick-\nson R Nascimento. Learning to dance: A graph convolu-\ntional adversarial network to generate realistic dance mo-\ntions from audio. Computers & Graphics, 94:11\u201321. 3\n[22] Ylva Ferstl and Rachel McDonnell. Investigating the use of\nrecurrent motion modelling for speech gesture generation. In\nProceedings of the 18th International Conference on Intelli-\ngent Virtual Agents, pages 93\u201398, 2018. 3\n[23] Ylva Ferstl, Michael Neff, and Rachel McDonnell. Adver-\nsarial gesture generation with realistic gesture phasing. Com-\nputers & Graphics, 2020. 3\n[24] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-\ntendra Malik. Recurrent network models for human dynam-\nics. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 4346\u20134354, 2015. 3\n[25] Aphrodite Galata, Neil Johnson, and David Hogg. Learning\nvariable-length markov models of behavior. Computer Vision\nand Image Understanding, 81(3):398\u2013413, 2001. 2\n[26] Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenen-\nbaum, and Antonio Torralba. Foley music: Learning to gen-\nerate music from videos. arXiv preprint arXiv:2007.10984,\n2020. 3\n[27] Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges.\nLearning human motion models for long-term predictions.\nIn 2017 International Conference on 3D Vision (3DV), pages\n458\u2013466. IEEE, 2017. 3\n[28] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, An-\ndrew Owens, and Jitendra Malik. Learning individual styles\nof conversational gesture. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n3497\u20133506, 2019. 3\n[29] Deepak Gopinath and Jungdam Won. fairmotion - tools to\nload, process and visualize motion capture data.\nGithub,\n2020. 6\n\n\n[30] Rachel Heck and Michael Gleicher.\nParametric motion\ngraphs. In Proceedings of the 2007 symposium on Interactive\n3D graphics and games, pages 129\u2013136, 2007. 2\n[31] Alejandro Hernandez, Jurgen Gall, and Francesc Moreno-\nNoguer. Human motion prediction via spatio-temporal in-\npainting. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 7134\u20137143, 2019. 2\n[32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in neural information processing systems,\npages 6626\u20136637, 2017. 6\n[33] Daniel Holden, Taku Komura, and Jun Saito.\nPhase-\nfunctioned neural networks for character control.\nACM\nTransactions on Graphics (TOG), 36(4):1\u201313, 2017. 3\n[34] Daniel Holden, Jun Saito, and Taku Komura. A deep learning\nframework for character motion synthesis and editing. ACM\nTransactions on Graphics (TOG), 35(4):1\u201311, 2016. 2\n[35] Daniel Holden, Jun Saito, Taku Komura, and Thomas\nJoyce. Learning motion manifolds with convolutional au-\ntoencoders.\nIn SIGGRAPH Asia 2015 Technical Briefs,\npages 1\u20134. 2015. 2\n[36] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,\nand Daxin Jiang. Dance revolution: Long-term dance gen-\neration with music via curriculum learning. In International\nConference on Learning Representations, 2021. 3, 6, 7, 8\n[37] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.\nBlack, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial\nposer learning to reconstruct human pose from sparseinertial\nmeasurements in real time. ACM Transactions on Graphics,\n(Proc. SIGGRAPH Asia), 37(6):185:1\u2013185:15, Nov. 2018. 3\n[38] Vladimir Iashin and Esa Rahtu. Multi-modal dense video\ncaptioning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pages\n958\u2013959, 2020. 3\n[39] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6m: Large scale datasets and predic-\ntive methods for 3d human sensing in natural environments.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 36(7):1325\u20131339, jul 2014. 3, 4, 5\n[40] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh\nSaxena. Structural-rnn: Deep learning on spatio-temporal\ngraphs. In Proceedings of the ieee conference on computer\nvision and pattern recognition, pages 5308\u20135317, 2016. 3\n[41] Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J Weiss,\nYuan Cao, Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo,\nand Yonghui Wu.\nLeveraging weakly supervised data to\nimprove end-to-end speech-to-text translation. In ICASSP\n2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 7180\u20137184.\nIEEE, 2019. 3\n[42] Hsuan-Kai Kao and Li Su.\nTemporally guided music-to-\nbody-movement generation. In Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 147\u2013155,\n2020. 3\n[43] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki\nHori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki,\nNelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei\nWang, et al. A comparative study on transformer vs rnn in\nspeech applications. In 2019 IEEE Automatic Speech Recog-\nnition and Understanding Workshop (ASRU), pages 449\u2013\n456. IEEE, 2019. 3\n[44] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 3128\u20133137, 2015. 3\n[45] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[46] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\nBlack.\nVibe: Video inference for human body pose and\nshape estimation, 2019. 5\n[47] Lucas Kovar, Michael Gleicher, and Fr\u00b4ed\u00b4eric Pighin. Motion\ngraphs. In ACM SIGGRAPH 2008 classes, pages 1\u201310. 2008.\n2\n[48] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-captioning events in videos. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 706\u2013715, 2017. 3\n[49] Jogendra Nath Kundu, Himanshu Buckchash, Priyanka\nMandikal, Anirudh Jamkhandi, Venkatesh Babu RAD-\nHAKRISHNAN, et al. Cross-conditioned recurrent networks\nfor long-term synthesis of inter-person human motion inter-\nactions.\nIn Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 2724\u20132733,\n2020. 3\n[50] Kimerer LaMothe. The dancing species: how moving to-\ngether in time helps make us human. Aeon, June 2019. 1\n[51] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun\nWang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.\nDancing to music, 2019. 3, 7\n[52] Juheon Lee, Seohyun Kim, and Kyogu Lee. Listen to dance:\nMusic-driven choreography generation using autoregressive\nencoder-decoder network. arXiv preprint arXiv:1811.00818,\n2018. 3\n[53] Jehee Lee and Sung Yong Shin. A hierarchical approach to\ninteractive motion editing for human-like \ufb01gures. In Pro-\nceedings of the 26th annual conference on Computer graph-\nics and interactive techniques, pages 39\u201348, 1999. 2\n[54] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang.\nEntan-\ngled transformer for image captioning. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n8928\u20138937, 2019. 3\n[55] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang,\nSanja Fidler, and Hao Li. Learning to generate diverse dance\nmotions with transformer. arXiv preprint arXiv:2008.08171,\n2020. 3, 5, 6, 7, 8\n[56] Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, Zeng Huang,\nand Hao Li.\nAuto-conditioned recurrent networks for ex-\ntended complex human motion synthesis. ICLR, 2018. 3\n[57] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. SIGGRAPH Asia, 2015. 4\n\n\n[58] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural baby talk. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7219\u20137228,\n2018. 3\n[59] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black. Amass: Archive of mo-\ntion capture as surface shapes. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 5442\u2013\n5451, 2019. 3, 4\n[60] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis,\nMatt McVicar, Eric Battenberg, and Oriol Nieto.\nlibrosa:\nAudio and music signal analysis in python. In Proceedings\nof the 14th python in science conference, volume 8, 2015. 6,\n7\n[61] Dirk Moelants. Dance music, movement and tempo pref-\nerences. In Proceedings of the 5th Triennial ESCOM Con-\nference, pages 649\u2013652. Hanover University of Music and\nDrama, 2003. 4\n[62] Meinard M\u00a8uller, Tido R\u00a8oder, and Michael Clausen. Ef\ufb01cient\ncontent-based retrieval of motion capture data. In ACM SIG-\nGRAPH 2005 Papers, pages 677\u2013685. 2005. 6\n[63] Kensuke Onuma, Christos Faloutsos, and Jessica K Hodgins.\nFmdistance: A fast and effective distance function for mo-\ntion capture data. In Eurographics (Short Papers), pages 83\u2013\n86, 2008. 6\n[64] Katherine Pullen and Christoph Bregler.\nAnimating by\nmulti-level sampling. In Proceedings Computer Animation\n2000, pages 36\u201342. IEEE, 2000. 2\n[65] Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang,\nand Wenjun Zeng. Cross view fusion for 3d human pose\nestimation. In International Conference on Computer Vision\n(ICCV), 2019. 5\n[66] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training, 2018. 4, 5, 7\n[67] Xuanchi Ren, Haoran Li, Zijian Huang, and Qifeng Chen.\nMusic-oriented dance video synthesis with pose perceptual\nloss. arXiv preprint arXiv:1912.06606, 2019. 3\n[68] Xuanchi Ren, Haoran Li, Zijian Huang, and Qifeng Chen.\nSelf-supervised dance video synthesis conditioned on music.\nIn Proceedings of the 28th ACM International Conference on\nMultimedia, pages 46\u201354, 2020. 3\n[69] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou\nZhao, and Tie-Yan Liu. Fastspeech: Fast, robust and con-\ntrollable text to speech. In Advances in Neural Information\nProcessing Systems, pages 3171\u20133180, 2019. 3\n[70] Matteo Ruggero Ronchi and Pietro Perona. Benchmarking\nand error diagnosis in multi-instance pose estimation. In The\nIEEE International Conference on Computer Vision (ICCV),\nOct 2017. 4\n[71] Takaaki Shiratori, Atsushi Nakazawa, and Katsushi Ikeuchi.\nDancing-to-music character animation. In Computer Graph-\nics Forum, volume 25, pages 449\u2013458. Wiley Online Library,\n2006. 3\n[72] Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira\nKemelmacher-Shlizerman. Audio to body dynamics. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 7574\u20137583, 2018. 3\n[73] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito.\nNeural state machine for character-scene interactions. ACM\nTrans. Graph., 38(6):209\u20131, 2019. 3\n[74] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Za-\nman. Local motion phases for learning multi-contact char-\nacter movements. ACM Transactions on Graphics (TOG),\n39(4):54\u20131, 2020. 3\n[75] Statista.\nhttps://www.statista.\ncom/statistics/249396/\ntop-youtube-videos-views/, 2020.\nAccessed:\n2020-11-09. 1\n[76] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia\nSchmid.\nLearning video representations using contrastive\nbidirectional transformer. arXiv preprint arXiv:1906.05743,\n2019. 3\n[77] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 7464\u2013\n7473, 2019. 3\n[78] Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan S\nKankanhalli, Weidong Geng, and Xiangdong Li. Deepdance:\nMusic-to-dance motion choreography with adversarial learn-\ning. IEEE Transactions on Multimedia, 2020. 3, 4\n[79] Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody:\nAn lstm-autoencoder approach to music-oriented dance syn-\nthesis. In Proceedings of the 26th ACM international confer-\nence on Multimedia, pages 1598\u20131606, 2018. 3, 4\n[80] Graham W Taylor and Geoffrey E Hinton. Factored con-\nditional restricted boltzmann machines for modeling motion\nstyle. In Proceedings of the 26th annual international con-\nference on machine learning, pages 1025\u20131032, 2009. 2\n[81] Purva Tendulkar, Abhishek Das, Aniruddha Kembhavi, and\nDevi Parikh.\nFeel the music: Automatically generating a\ndance for an input song. arXiv preprint arXiv:2006.11905,\n2020. 3\n[82] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,\nand Masataka Goto. Aist dance video database: Multi-genre,\nmulti-dancer, and multi-camera database for dance informa-\ntion processing.\nIn Proceedings of the 20th International\nSociety for Music Information Retrieval Conference, ISMIR\n2019, pages 501\u2013510, Delft, Netherlands, Nov. 2019. 2, 3,\n12, 13\n[83] Jean-Marc Valin and Jan Skoglund. Lpcnet: Improving neu-\nral speech synthesis through linear prediction. In ICASSP\n2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 5891\u20135895.\nIEEE, 2019. 3\n[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998\u20136008, 2017. 3,\n4\n[85] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Don-\nahue, Raymond Mooney, Trevor Darrell, and Kate Saenko.\nSequence to sequence-video to text. In Proceedings of the\nIEEE international conference on computer vision, pages\n4534\u20134542, 2015. 3\n\n\n[86] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Mar-\ncus Rohrbach, Raymond Mooney, and Kate Saenko. Trans-\nlating videos to natural language using deep recurrent neural\nnetworks. arXiv preprint arXiv:1412.4729, 2014. 3\n[87] Ruben Villegas, Jimei Yang, Duygu Ceylan, and Honglak\nLee.\nNeural kinematic networks for unsupervised motion\nretargetting. In CVPR, 2018. 3\n[88] Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang,\nand Juan Carlos Niebles. Imitation learning for human pose\nprediction. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 7124\u20137133, 2019. 3\n[89] Haoming Xu, Runhao Zeng, Qingyao Wu, Mingkui Tan,\nand Chuang Gan. Cross-modal relation-aware networks for\naudio-visual event localization. In Proceedings of the 28th\nACM International Conference on Multimedia, pages 3893\u2013\n3901, 2020. 3\n[90] Nelson Yalta, Shinji Watanabe, Kazuhiro Nakadai, and Tet-\nsuya Ogata. Weakly-supervised deep recurrent neural net-\nworks for basic dance step generation. In 2019 International\nJoint Conference on Neural Networks (IJCNN), pages 1\u20138.\nIEEE, 2019. 3\n[91] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas,\nChristopher Pal, Hugo Larochelle, and Aaron Courville. De-\nscribing videos by exploiting temporal structure. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 4507\u20134515, 2015. 3\n[92] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo\nMeng, and Yanfeng Wang. Choreonet: Towards music to\ndance synthesis with choreographic action unit. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 744\u2013752, 2020. 3\n[93] Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, and Wei\nXu. Video paragraph captioning using hierarchical recurrent\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4584\u20134593,\n2016. 3\n[94] He Zhang, Sebastian Starke, Taku Komura, and Jun Saito.\nMode-adaptive neural networks for quadruped motion con-\ntrol.\nACM Transactions on Graphics (TOG), 37(4):1\u201311,\n2018. 3\n[95] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 8739\u2013\n8748, 2018. 3\n[96] Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and\nYangang Wang. Music2dance: Music-driven dance genera-\ntion using wavenet. arXiv preprint arXiv:2002.03761, 2020.\n3, 4, 6, 8\n[97] Wenlin Zhuang, Yangang Wang, Joseph Robinson, Con-\ngyi Wang, Ming Shao, Yun Fu, and Siyu Xia.\nTowards\n3d dance motion synthesis and control.\narXiv preprint\narXiv:2006.05743, 2020. 3",
      "type": "references",
      "word_count": 2879
    },
    {
      "title": "Appendix",
      "content": "A. AIST++ Dataset Details\n3D Reconstruction\nHere we describe how we reconstruct\n3D motion from the AIST dataset.\nAlthough the AIST\ndataset contains multi-view videos, they are not calibrated\nmeaning their camera intrinsic and extrinsic parameters are\nnot available. Without camera parameters, it is not trivial\nto automatically and accurately reconstruct the 3D human\nmotion. We start with 2D human pose detection [?] and\nmanually initialized the camera parameters.\nOn this we\napply bundle adjustment [?] to re\ufb01ne the camera parame-\nters. With the improved camera parameters, the 3D joint\nlocations \u02c6J \u2208RM\u00d73(M = 17) are then triangulated from\nthe multi-view 2D human pose keypoints locations. During\nthe triangulation phase, we introduce temporal smoothness\nand bone length constraints to improve the quality of the\nreconstructed 3D joint locations. We further \ufb01t SMPL hu-\nman body model [?] to the triangulated joint locations \u02c6J\nby minimizing an objective with respect to \u0398 = {\u03b8i}M\ni ,\nglobal scale parameter \u03b1 and global transformation \u03b3 for\neach frame: min\u0398,\u03b3,\u03b1\nPM\ni=1 \u2225\u02c6J \u2212J(\u03b8i, \u03b2, \u03b3, \u03b1)\u22252. We \ufb01x\n\u03b2 to the average shape as the problem is under-constrained\nfrom 3D joint locations alone.\nStatistics\nWe show the detailed statistics of our AIST++\ndataset in Table 5.\nThanks to the AIST Dance Video\nDatabase [82], our dataset contains in total 5.2-hour (1.1M\nframe, 1408 sequences) of 3D dance motion accompanied\nwith music. The dataset covers 10 dance genre (shown in\nFigure 8) and 60 pieces of music. For each genre, there\nare 6 different pieces of music, ranging from 29 seconds to\n54 seconds long, and from 80 BPM to 130 BPM (except for\nHouse genre which is 110 BPM to 135 BPM). Among those\nmotion sequences for each genre, 120 (85%) of them are\nbasic choreographies and 21 (15%) of them are advanced.\nAdvanced choreographies are longer and more complicated\ndances improvised by the dancers. Note for the basic dance\nmotion, dancers are asked to perform the same choreog-\nraphy on all the 6 pieces of music with different speed to\nfollow different music BPMs. So the total unique chore-\nographies in for each genre is 120/6 + 21 = 41. In our\nexperiments we split the AIST++ dataset such that there is\nno overlap between train and test for both music and chore-\nographies (see Sec. 5.2.1 in the paper).\nValidation\nAs described in Sec. 5.1 in the paper, we val-\nidate the quality of our reconstructed 3D motion by cal-\nculating the overall MPJPE-2D (in pixel) between the re-\nprojected 2D keypoints and the detected 2D keypoints with\nhigh con\ufb01dence (> 0.5).\nWe provide here the distribu-\ntion of MPJPE-2D among all video sequences (Figure 9).\n\n\nGenres\nMusics\nMusic Tempo\nMotions\nChoreographs\nMotion Duration (sec.)\nTotal Seconds\nballet jazz\n6\n80 - 130\n141\n85% basic +\n15% advanced\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1910.8\nstreet jazz\n6\n80 - 130\n141\n7.4 - 12.0 basic / 14.9 - 48.0 adv.\n1875.3\nkrump\n6\n80 - 130\n141\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1904.3\nhouse\n6\n110 - 135\n141\n7.1 - 8.7 basic / 28.4 - 34.9 adv.\n1607.6\nLA-style hip-hop\n6\n80 - 130\n141\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1935.8\nmiddle hip-hop\n6\n80 - 130\n141\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1934.0\nwaack\n6\n80 - 130\n140\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1897.1\nlock\n6\n80 - 130\n141\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1898.5\npop\n6\n80 - 130\n140\n7.4 - 12.0 basic / 29.5 - 48.0 adv.\n1872.9\nbreak\n6\n80 - 130\n141\n7.4 - 12.0 basic / 23.8 - 48.0 adv.\n1858.3\ntotal\n60\n1408\n18694.6\nTable 5: AIST++ Dataset Statistics. AIST++ is built upon a subset of AIST database [82] that contains single-person dance.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized distance\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPercent. of correct keypoints\nFigure 7: PCKh Metric on AIST++.\nWe analyze the\nPCKh (percentage of correct keypoints) metric between\nre-projected 2D keypoints and detected 2D keypoints on\nAIST++. Averaged PCKh@0.5 is 98.4% on all joints shows\nthat our reconstructed 3D keypoints are highly consistent\nwith the predicted 2D keypoints.\nwaack\nbreaking\nhouse\nmiddle hip-hop\npop\nlock\nballet jazz\nstreet jazz\nkrump\nLA-style hip-hop\nFigure 8: AIST++ Motion Diversity Visualization. Here\nwe show the 10 types of 3D human dance motion in our\ndataset.\nMoreover, we also analyze the PCKh metric with various\nthresholds on the AIST++, which measures the consistency\nbetween the re-projected and detected 2D keypoints. Av-\n0\n6\n12\n18\n24\n30\nReprejection error (in pixels)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nNumber of videos\nFigure 9: MPJPE-2D Distribution on AIST++. We an-\nalyze the distribution of MPJPE-2D among all video se-\nquences on 1920x1080 resolution. MPJPE-2D is calculated\nbetween the re-projected 2D keypoints and the detected 2D\nkeypoints. Over 86% of the videos have less than average\n10 pixels of error.\neraged PCKh@0.5 is 98.4% on all joints shows that our\nreconstructed 3D keypoints are highly consistent with the\ndetected 2D keypoints.\nB. User Study Details\nB.1. Comparison User Study\nAs mentioned in Sec. 5.2.5 in the main paper, we qualita-\ntively compare our generated results with several baselines\nin a user study. Here we describe the details of this user\nstudy. Figure 11 shows the interface that we developed for\nthis user study. We visualize the dance motion using stick-\nman and conduct side-by-side comparison between our gen-\nerated results and the baseline methods. The left-right order\nis randomly shuf\ufb02ed for each video to make sure that the\nparticipants have absolutely no idea which is ours. Each\nvideo is 10-second long, accompanied with the music. The\nquestion we ask each participant is \u201cwhich person is danc-\n\n\nHow many years have you been dancing?\nHow often do you watch dance videos?\nFigure 10: Participant Demography of the Comparison User Study.\ning more to the music? LEFT or RIGHT\u201d, and the answers\nare collected through a Google Form. At the end of this user\nstudy, we also have an exit survey to ask for the dance expe-\nrience of the participants. There are two questions: \u201cHow\nmany years have you been dancing?\u201d, and \u201cHow often do\nyou watch dance videos?\u201d. Figure 10 shows that our par-\nticipants ranges from professional dancers to people rarely\ndance, with majority with at least 1 year of dance experi-\nence.\n\n\nFigure 11: User study interface. The interface of our User study. We ask each participant to watch 10 videos and answer\nthe question \u201dwhich person is dancing more to the music? LEFT or RIGHT\u201d.\n==================================================",
      "type": "appendix",
      "word_count": 1123
    },
    {
      "title": "ABSTRACT",
      "content": "==================================================\n\nWe present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 se- quences, covering 10 dance genres with multi-view videos with known camera poses\u2014the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N future motions. We empirically show that these changes are key factors in gen- erating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive ex- periments on AIST++ with user studies, where our method",
      "type": "abstract",
      "word_count": 170
    }
  ],
  "metadata": {
    "source_file": "data/extracted_texts\\Li2021_AI_Choreographer_Music_Conditioned_3D_Dance_Genera_extracted.txt",
    "total_sections": 8
  }
}