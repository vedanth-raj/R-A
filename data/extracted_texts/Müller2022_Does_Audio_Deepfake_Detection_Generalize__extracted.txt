--- Page 1 ---
Does Audio Deepfake Detection Generalize?
Nicolas M. M¨uller1, Pavel Czempin2, Franziska Dieckmann2,
Adam Froghyar3, Konstantin B¨ottinger1
1Fraunhofer AISEC
2Technical University Munich
3why do birds GmbH
nicolas.mueller@aisec.fraunhofer.de
Abstract
Current text-to-speech algorithms produce realistic fakes of hu-
man voices, making deepfake detection a much-needed area of
research. While researchers have presented various deep learn-
ing models for audio spoofs detection, it is often unclear exactly
why these architectures are successful: Preprocessing steps, hy-
perparameter settings, and the degree of fine-tuning are not con-
sistent across related work. Which factors contribute to success,
and which are accidental?
In this work, we address this problem: We systematize au-
dio spoofing detection by re-implementing and uniformly evalu-
ating twelve architectures from related work. We identify over-
arching features for successful audio deepfake detection, such
as using cqtspec or logspec features instead of melspec features,
which improves performance by 37% EER on average, all other
factors constant.
Additionally, we evaluate generalization capabilities: We
collect and publish a new dataset consisting of 37.9 hours of
found audio recordings of celebrities and politicians, of which
17.2 hours are deepfakes. We find that related work performs
poorly on such real-world data (performance degradation of up
to one thousand percent).
This could suggest that the com-
munity has tailored its solutions too closely to the prevailing
ASVspoof benchmark and that deepfakes are much harder to
detect outside the lab than previously thought.
1. Introduction
Modern text-to-speech synthesis (TTS) is capable of realis-
tic fakes of human voices, also known as audio deepfakes
or spoofs. While there are many ethical applications of this
technology, there is also a serious risk of malicious use. For
example, TTS technology enables the cloning of politicians’
voices [1, 2], which poses a variety of risks to society, including
the spread of misinformation.
Reliable detection of speech spoofing can help mitigate
such risks and is therefore an active area of research. However,
since the technology to create audio deepfakes has only been
available for a few years (see Wavenet [3] and Tacotron [4],
published in 2016/17), audio spoof detection is still in its in-
fancy. While many approaches have been proposed (cf.
Sec-
tion 2), it is still difficult to understand why some of the models
work well: Each work uses different feature extraction tech-
niques, preprocessing steps, hyperparameter settings, and fine-
tuning. Which are the main factors and drivers for models to
perform well? What can be learned in principle for the devel-
opment of such systems?
Furthermore, the evaluation of spoof detection models has
so far been performed exclusively on the ASVspoof dataset [5,
6], which means that the reported performance of these mod-
els is based on a limited set of TTS synthesis algorithms.
ASVspoof is based on the VCTK dataset [7], which exclusively
features professional speakers and has been recorded in a stu-
dio environment, using a semi-anechoic chamber. What can
we expect from audio spoof detection trained on this dataset?
Is it capable of detecting realistic, unseen, ‘in-the-wild’ audio
spoofs like those encountered on social media?
To answer these questions, this paper presents the following
contributions:
• We reimplement twelve of the most popular architec-
tures from related work and evaluate them according to a
common standard. We systematically exchange compo-
nents to attribute performance reported in related work
to either model architecture, feature extraction, or data
preprocessing techniques. In this way, we identify fun-
damental properties for well-performing audio deepfake
detection.
• To investigate the applicability of related work in the real
world, we introduce a new audio deepfake dataset1. We
collect 17.2 hours of high-quality audio deepfakes and
20.7 hours of of authentic material from 58 politicians
and celebrities.
• We show that established models generally perform
poorly on such real-world data. This discrepancy be-
tween reported and actual generalization ability suggests
that the detection of audio fakes is a far more difficult
challenge than previously thought.
2. Related Work
2.1. Model Architectures
There is a significant body of work on audio spoof detection,
driven largely by the ASVspoof challenges and datasets [5, 6].
In this section, we briefly present the architectures and models
used in our evaluation in Section 5.
LSTM-based models. Recurrent architectures are a natu-
ral choice in the area of language processing, with numerous
related work utilizing such models [8, 9, 10, 11]. As a base-
line for evaluating this approach, we implement a simple LSTM
model: it consists of three LSTM layers followed by a single
linear layer. The output is averaged over the time dimension to
obtain a single embedding vector.
LCNN. Another common architecture for audio spoof
detection are LCNN-based learning models such as LCNN,
LCNN-Attention, and LCNN-LSTM [12, 13, 14].
LC-
NNs combine convolutional layers with Max-Feature-Map
activations to create ‘light’ convolutional neural networks.
LCNN-Attention has an added single-head-attention pool-
ing layer, while LCNN-LSTM uses a Bi-LSTM layer and a skip
connection.
MesoNet. MesoNet is based on the Meso-4 [15] archi-
tecture, which was originally used for detecting facial video
1https://deepfake-total.com/in the wild
arXiv:2203.16263v4  [cs.SD]  27 Aug 2024


--- Page 2 ---
deepfakes. It uses 4 convolutional layers in addition to Batch
Normalization, Max Pooling, and a fully connected classifier.
MesoInception.
Based on the facial deepfake detector
Meso-Inception-4 [15], MesoInception extends the Meso-4
architecture with Inception blocks [16].
ResNet18. Residual Networks were first used for audio
deepfake detection by [17], and continue to be employed [18,
19]. This architecture, first introduced in the computer vision
domain [20], uses convolutional layers and shortcut connec-
tions, which avoids the vanishing gradient problem and allows
to design especially deep networks (18 layers for ResNet18).
Transformer. The Transformer architecture has also found
its way into the field of audio spoof detection [21]. We use
four self-attention layers with 256 hidden dimensions and skip-
connections, and encode time with positional encodings [22].
CRNNSpoof. This end-to-end architecture combines 1D
convolutions with recurrent layers to learn features directly
from raw audio samples [9].
RawNet2 [23] is another end-to-end model.
It employs
Sinc-Layers [24], which correspond to rectangular band-pass
filters, to extract information directly from raw waveforms.
RawPC is an end-to-end model which also uses Sinc-layers
to operate directly on raw wavforms. The architecture is found
via differentiable architecture search [25].
RawGAT-ST, a spectro-temporal graph attention network
(GAT), trained in an end-to-end fashion. It introduces spectral
and temporal sub-graphs and a graph pooling strategy, and re-
ports state-of-the-art spoof detection capabilities [26], which we
can verify experimentally, c.f. Table 1.
3. Datasets
To train and evaluate our models, we use the ASVspoof 2019
dataset [5], in particular its Logical Access (LA) part. It con-
sists of audio files that are either real (i.e., authentic recordings
of human speech) or fake (i.e., synthesized or faked audio). The
spoofed audio files are from 19 different TTS synthesis algo-
rithms. From a spoofing detection point of view, ASVspoof
considers synthetic utterances as a threat to the authenticity of
the human voice, and therefore labels them as ‘attacks’. In total,
there are 19 different attackers in the ASVspoof 2019 dataset,
labeled A1 - A19. For each attacker, there are 4914 synthetic
audio recordings and 7355 real samples.
This dataset is ar-
guably the best known audio deefake dataset used by almost
all related work.
In order to evaluate our models on realistic unseen data
in-the-wild, we additionally create and publish a new audio
deefake dataset, c.f. Figure 1. It consists of 37.9 hours of audio
clips that are either fake (17.2 hours) or real (20.7 hours). We
feature English-speaking celebrities and politicians, both from
present and past2. The fake clips are created by segmenting
219 of publicly available video and audio files that explicitly
advertise audio deepfakes. Since the speakers talk absurdly and
out-of-character (‘Donald Trump reads Star Wars’), it is easy to
verify that the audio files are really spoofed. We then manually
collect corresponding genuine instances from the same speak-
ers using publicly available material such as podcasts, speeches,
etc. We take care to include clips where the type of speaker,
style, emotions, etc. are similar to the fake (e.g., for a fake
speech by Barack Obama, we include an authentic speech and
try to find similar values for background noise, emotions, dura-
tion, etc.). The clips have an average length of 4.3 seconds and
2records available at deepfake-total.com/in the wild
Figure 1: Schematics of our collected dataset. For n = 58
celebrities and politicians, we collected both bona-fide and
spoofed audio (represented by blue and red boxes per speaker).
In total, we collected 20.8 hours of bona-fide and 17.2 hours of
spoofed audio. On average, there are 23 minutes of bona-fide
and 18 minutes of spoofed audio per speaker.
are converted to ‘wav’ after downloading. All recordings were
downsampled to 16 kHz (the highest common frequency in the
original recordings). Clips were collected from publicly avail-
able sources such as social networks and popular video sharing
platforms. This dataset is intended as evaluation data: it allows
evaluation of a model’s cross-database capabilities on a realistic
use case.
4. Experimental Setup
4.1. Training and Evaluation
4.1.1. Hyper Parameters
We train all of our models using a cross-entropy loss with a log-
Softmax over the output logits. We choose the Adam [27] opti-
mizer. We initialize the learning rate at 0.0001 and use a learn-
ing rate scheduler. We train for 100 epochs with early stopping
using a patience of five epochs.
4.1.2. Train and Evaluation Data Splits
We train our models on the ‘train’ and ‘dev’ parts of the
ASVSpoof 2019 Logical Access (LA) dataset part [5]. This
is consistent with most related work and also with the eval-
uation procedure of the ASVspoof 2019 Challenge. We test
against two evaluation datasets. As in-domain evaluation data,
we use the ‘eval’ split of ASVspoof 2019. This split contains
unseen attacks, i.e., attacks not seen during training. However,
the evaluation audios share certain properties with the training
data [28], so model generalization cannot be assessed using the
‘eval’ split of ASVspoof 2019 alone. This motivates the use of
our proposed ‘in-the-wild’ dataset, see Section 3, as unknown
out-of-domain evaluation data.
4.1.3. Evaluation metrics
We report both the equal-error rate (EER) and the tandem de-
tection cost function (t-DCF) [29] on the ASVspoof 2019 ‘eval’
data. For consistency with the related work, we use the origi-
nal implementation of the t-DCF as provided for the ASVspoof
2019 challenge [30]. For our proposed dataset, we report only
the EER. This is because t-DCF scores require the false alarm
and miss costs, which are available only for ASVspoof.
4.2. Feature Extraction
Several
architectures
used
in
this
work
require
pre-
processing the audio data with a feature extractor (LCNN,
LCNN-Attention,
LCNN-LSTM,
LSTM,
MesoNet,
MesoInception, ResNet18, Transformer). We evalu-


--- Page 3 ---
ASVspoof19 eval
In-the-Wild Data
Model Name
Feature Type
Input Length
EER%
t-DCF
EER%
LCNN
cqtspec
Full
6.354±0.39
0.174±0.03
65.559±11.14
LCNN
cqtspec
4s
25.534±0.10
0.512±0.00
70.015±4.74
LCNN
logspec
Full
7.537±0.42
0.141±0.02
72.515±2.15
LCNN
logspec
4s
22.271±2.36
0.377±0.01
91.110±2.17
LCNN
melspec
Full
15.093±2.73
0.428±0.05
70.311±2.15
LCNN
melspec
4s
30.258±3.38
0.503±0.04
81.942±3.50
LCNN-Attention
cqtspec
Full
6.762±0.27
0.178±0.01
66.684±1.08
LCNN-Attention
cqtspec
4s
23.228±3.98
0.468±0.06
75.317±8.25
LCNN-Attention
logspec
Full
7.888±0.57
0.180±0.05
77.122±4.91
LCNN-Attention
logspec
4s
14.958±2.37
0.354±0.03
80.651±6.14
LCNN-Attention
melspec
Full
13.487±5.59
0.374±0.14
70.986±9.73
LCNN-Attention
melspec
4s
19.534±2.57
0.449±0.02
85.118±1.01
LCNN-LSTM
cqtspec
Full
6.228±0.50
0.113±0.01
61.500±1.37
LCNN-LSTM
cqtspec
4s
20.857±0.14
0.478±0.01
72.251±2.97
LCNN-LSTM
logspec
Full
9.936±1.74
0.158±0.01
79.109±0.84
LCNN-LSTM
logspec
4s
13.018±3.08
0.330±0.05
79.706±15.80
LCNN-LSTM
melspec
Full
9.260±1.33
0.240±0.04
62.304±0.17
LCNN-LSTM
melspec
4s
27.948±4.64
0.483±0.03
82.857±3.49
LSTM
cqtspec
Full
7.162±0.27
0.127±0.00
53.711±11.68
LSTM
cqtspec
4s
14.409±2.19
0.382±0.05
55.880±0.88
LSTM
logspec
Full
10.314±0.81
0.160±0.00
73.111±2.52
LSTM
logspec
4s
23.232±0.32
0.512±0.00
78.071±0.49
LSTM
melspec
Full
16.216±2.92
0.358±0.00
65.957±7.70
LSTM
melspec
4s
37.463±0.46
0.553±0.01
64.297±2.23
MesoInception
cqtspec
Full
11.353±1.00
0.326±0.03
50.007±14.69
MesoInception
cqtspec
4s
21.973±4.96
0.453±0.09
68.192±12.47
MesoInception
logspec
Full
10.019±0.18
0.238±0.02
37.414±9.16
MesoInception
logspec
4s
16.377±3.72
0.375±0.09
72.753±6.62
MesoInception
melspec
Full
14.058±5.67
0.331±0.11
61.996±12.65
MesoInception
melspec
4s
21.484±3.51
0.408±0.03
51.980±15.32
MesoNet
cqtspec
Full
7.422±1.61
0.219±0.07
54.544±11.50
MesoNet
cqtspec
4s
20.395±2.03
0.426±0.06
65.928±2.57
MesoNet
logspec
Full
8.369±1.06
0.170±0.05
46.939±5.81
MesoNet
logspec
4s
11.124±0.79
0.263±0.03
80.707±12.03
MesoNet
melspec
Full
11.305±1.80
0.321±0.06
58.405±11.28
MesoNet
melspec
4s
21.761±0.26
0.467±0.00
64.415±15.68
ResNet18
cqtspec
Full
6.552±0.49
0.140±0.01
49.759±0.17
ResNet18
cqtspec
4s
18.378±1.76
0.432±0.07
61.827±7.46
ResNet18
logspec
Full
7.386±0.42
0.139±0.02
80.212±0.23
ResNet18
logspec
4s
15.521±1.83
0.387±0.02
88.729±2.88
ResNet18
melspec
Full
21.658±2.56
0.551±0.04
77.614±1.47
ResNet18
melspec
4s
28.178±0.33
0.489±0.01
83.006±7.17
Transformer
cqtspec
Full
7.498±0.34
0.129±0.01
43.775±2.85
Transformer
cqtspec
4s
11.256±0.07
0.329±0.00
48.208±1.49
Transformer
logspec
Full
9.949±1.77
0.210±0.06
64.789±0.88
Transformer
logspec
4s
13.935±1.70
0.320±0.03
44.406±2.17
Transformer
melspec
Full
20.813±6.44
0.394±0.10
73.307±2.81
Transformer
melspec
4s
26.495±1.76
0.495±0.00
68.407±5.53
CRNNSpoof
raw
Full
15.658±0.35
0.312±0.01
44.500±8.13
CRNNSpoof
raw
4s
19.640±1.62
0.360±0.04
41.710±4.86
RawNet2
raw
Full
3.154±0.87
0.078±0.02
37.819±2.23
RawNet2
raw
4s
4.351±0.29
0.132±0.01
33.943±2.59
RawPC
raw
Full
3.092±0.36
0.071±0.00
45.715±12.20
RawPC
raw
4s
3.067±0.91
0.097±0.03
52.884±6.08
RawGAT-ST
raw
Full
1.229±0.43
0.036±0.01
37.154±1.95
RawGAT-ST
raw
4s
2.297±0.98
0.074±0.03
38.767±1.28
Table 1: Full results of evaluation on the ASVspoof 2019 LA ‘eval’ data. We compare different model architectures against different
feature types and audio input lengths (4s, fixed-sized inputs vs. variable-length inputs). Results are averaged over three independent
trials with random initialization, and the standard deviation is reported. Best-performing configurations are highlighted in boldface.
When evaluating the models on our proposed ‘in-the-wild’ dataset, we see an increase in EER by up to 1000% compared to ASVspoof
2019 (rightmost column).


--- Page 4 ---
ASVspoof19 eval
In-the-Wild Data
Input Length
EER %
t-DCF
EER %
Full
9.85
0.22
60.10
4s
18.89
0.39
67.25
Table 2: Model performance averaged by input preprocess-
ing. Fixed-length, 4s inputs perform significantly worse on the
ASVspoof data and on the ‘in-the-wild’ dataset than variable-
length inputs. This suggests that related work using fixed-length
inputs may (unnecessarily) sacrifice performance.
ate these architectures on constant-Q transform (cqtspec [31]),
log spectrogram (logspec) and mel-scaled spectrogram (mel-
spec [32]) features (all of them 513-dimensional).
We use
Python, librosa [33] and scipy [34]. The rest of the models does
not rely on pre-processed data, but uses raw audio waveforms
as inputs.
4.3. Audio Input Length
Audio samples usually vary in length, which is also the case
for the data in ASVspoof 2019 and our proposed ‘in-the-wild’
dataset. While some models can accommodate variable-length
input (and thus also fixed-length input), many can not. We ex-
tend these by introducing a global averaging layer, which adds
such capability.
In our evaluation of fixed-length input, we chose a length
of four seconds, following [23]. If an input sample is longer, a
random four-second subset of the sample is used. If it is shorter,
the sample is repeated. To keep the evaluation fair, these shorter
samples are also repeated during the full-length evaluation. This
ensures that full-length input is never shorter than truncated in-
put, but always at least 4s.
5. Results
Table 1 shows the results of our experiments, where we evaluate
all models against all configurations of data preprocessing: we
train twelve different models, using one of four different feature
types, with two different ways of handling variable-length au-
dio. Each experiment is performed three times, using random
initialization. We report averaged EER and t-DCF, as well as
standard deviation. We observe that on ASVspoof, our imple-
mentations perform comparable to related work, with a margin
of approximately 2 −4% EER and 0.1 t-DCF. This is likely
because we do not fine-tune our models’ hyper-parameters.
5.1. Fixed vs. Variable Input Length
We analyze the effects of truncating the input signal to a fixed
length compared to using the full, unabridged audio. For all
models, performance decreases when the input is trimmed to
4s. Table 2 averages all results based on input length. We see
that average EER on ASVspoof drops from 19.89% to 9.85%
when the full-length input is used. These results show that a
four-second clip is insufficient for the model to extract useful in-
formation compared to using the full audio file as input. There-
fore, we propose not to use fixed-length truncated inputs, but to
provide the full audio file to the model. This may seem obvious,
but the numerous works that use fixed-length inputs [23, 25, 26]
suggest otherwise.
5.2. Effects of Feature Extraction Techniques
We discuss the effects of different feature preprocessing tech-
niques, c.f. 1: The ‘raw’ models outperform the feature-based
models, obtaining up to 1.2% EER on ASVspoof and 33.9%
EER on the ‘in-the-wild’ dataset (RawGAT-ST and RawNet2).
The spectrogram-based models perform slightly worse, achiev-
ing up to 6.3% EER on ASVspoof and 37.4% on the ‘in-the-
wild’ dataset (LCNN and MesoNet). The superiority of the
‘raw’ models is assumed to be due to finer feature-extraction
resolution than the spectogram-based models [26]. This has
lead recent research to focus largely on such raw-feature, end-
to-end models [25, 26].
Concerning the spectogram-based models, we observe that
melspec features are always outperformed by either cqtspec of
logspec. Simply replacing melspec with cqtspec increases the
average performance by 37%, all other factors constant.
5.3. Evaluation on ‘in-the-wild’ data
Especially interesting is the performance of the models on real-
world deepfake data. Table 1 shows the performance of our
models on the ‘in-the-wild’ dataset. We see that there is a large
performance gap between the ASVSpoof 2019 evaluation data
and our proposed ‘in-the-wild’ dataset. In general, the EER
values of the models deteriorate by about 200 to 1000 percent.
Often, the models do not perform better than random guessing.
To investigate this further, we train our best ‘in-the-wild’
model from Table 1, RawNet2 with 4s input length, on all
from ASVspoof 2019, i.e., the ‘train’, ‘dev’, and ‘eval’ splits.
We then re-evaluate on the ‘in-the-wild’ dataset to investigate
whether adding more ASVspoof training data improves out-of-
domain performance. We achieve 33.1 ± 0.2 % EER, i.e., no
improvement over training with only the ‘train’ and ‘dev’ data.
The inclusion of the ‘eval’ split does not seem to add much
information that could be used for real-world generalization.
This is plausible in that all splits of ASVspoof are fundamen-
tally based on the same dataset, VCTK, although the synthesis
algorithms and speakers differ between splits [5].
6. Conclusion
In this paper, we systematically evaluate audio spoof detection
models from related work according to common standards. In
addition, we present a new audio deefake dataset of ‘in-the-
wild’ audio spoofs that we use to evaluate the generalization
capabilities of related work in a real-world scenario.
We find that regardless of the model architecture, some pre-
processing steps are more successful than others. It turns out
that the use of cqtspec or logspec features consistently outper-
forms the use of melspec features in our comprehensive analy-
sis. Furthermore, we find that for most models, four seconds of
input audio does not saturate performance compared to longer
examples. Therefore, we argue that one should consider using
cqtspec features and unabridged input audio when designing au-
dio deepfake detection architectures.
Most importantly, however, we find that the ‘in-the-wild’
generalization capabilities of many models may have been over-
estimated. We demonstrate this by collecting our own audio
deepfake dataset and evaluating twelve different model archi-
tectures on it. Performance drops sharply, and some models
degenerate to random guessing.
It may be possible that the
community has tailored its detection models too closely to the
prevailing benchmark, ASVSpoof, and that deepfakes are much
harder to detect outside the lab than previously thought.


--- Page 5 ---
7. References
[1] “Audio deep fake: Demonstrator entwickelt am fraunhofer aisec -
youtube,” https://www.youtube.com/watch?v=MZTF0eAALmE,
(Accessed on 04/01/2021).
[2] “Deepfake video of volodymyr zelensky surrendering surfaces
on social media - youtube,” https://www.youtube.com/watch?v=
X17yrEV5sl4, (Accessed on 03/23/2022).
[3] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,
“Wavenet: A generative model for raw audio,” arXiv preprint
arXiv:1609.03499, 2016.
[4] Y. Wang,
R. J. Skerry-Ryan,
D. Stanton,
Y. Wu,
R. J.
Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio,
Q.
V.
Le,
Y.
Agiomyrgiannakis,
R.
Clark,
and
R.
A.
Saurous, “Tacotron: A fully end-to-end text-to-speech synthesis
model,” CoRR, vol. abs/1703.10135, 2017. [Online]. Available:
http://arxiv.org/abs/1703.10135
[5] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Delgado,
A. Nautsch, J. Yamagishi, N. Evans, T. Kinnunen, and K. A. Lee,
“Asvspoof 2019: Future horizons in spoofed and fake audio de-
tection,” arXiv preprint arXiv:1904.05441, 2019.
[6] A. Nautsch, X. Wang, N. Evans, T. H. Kinnunen, V. Vestman,
M. Todisco, H. Delgado, M. Sahidullah, J. Yamagishi, and K. A.
Lee, “ASVspoof 2019: Spoofing Countermeasures for the De-
tection of Synthesized, Converted and Replayed Speech,” vol. 3,
no. 2, pp. 252–265.
[7] J. Yamagishi, C. Veaux, and K. MacDonald, “CSTR VCTK Cor-
pus: English multi-speaker corpus for CSTR voice cloning toolkit
(version 0.92),” 2019.
[8] A. Gomez-Alanis, A. M. Peinado, J. A. Gonzalez, and A. M.
Gomez, “A Gated Recurrent Convolutional Neural Network for
Robust Spoofing Detection,” vol. 27, no. 12, pp. 1985–1999.
[9] A. Chintha, B. Thai, S. J. Sohrawardi, K. M. Bhatt, A. Hickerson,
M. Wright, and R. Ptucha, “Recurrent Convolutional Structures
for Audio Spoof and Video Deepfake Detection,” pp. 1–1.
[10] L. Zhang, X. Wang, E. Cooper, J. Yamagishi, J. Patino, and
N. Evans, “An initial investigation for detecting partially spoofed
audio,” arXiv preprint arXiv:2104.02518, 2021.
[11] S. Tambe, A. Pawar, and S. Yadav, “Deep fake videos identifica-
tion using ann and lstm,” Journal of Discrete Mathematical Sci-
ences and Cryptography, vol. 24, no. 8, pp. 2353–2364, 2021.
[12] X. Wang and J. Yamagishi. A Comparative Study on Recent Neu-
ral Spoofing Countermeasures for Synthetic Speech Detection.
[Online]. Available: http://arxiv.org/abs/2103.11326
[13] G. Lavrentyeva, S. Novoselov, E. Malykh, A. Kozlov, O. Kuda-
shev, and V. Shchemelinin, “Audio replay attack detection with
deep learning frameworks,” in Interspeech 2017.
ISCA, pp.
82–86. [Online]. Available: http://www.isca-speech.org/archive/
Interspeech 2017/abstracts/0360.html
[14] G.
Lavrentyeva,
S.
Novoselov,
A.
Tseren,
M.
Volkova,
A. Gorlanov, and A. Kozlov, “STC antispoofing systems for
the ASVspoof2019 challenge,” in Interspeech 2019.
ISCA,
pp. 1033–1037. [Online]. Available: http://www.isca-speech.org/
archive/Interspeech 2019/abstracts/1768.html
[15] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, “MesoNet:
A Compact Facial Video Forgery Detection Network,” in 2018
IEEE International Workshop on Information Forensics and Se-
curity (WIFS), pp. 1–7.
[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2015, pp. 1–9.
[17] M. Alzantot, Z. Wang, and M. B. Srivastava, “Deep Residual
Neural Networks for Audio Spoofing Detection,” in Interspeech
2019.
ISCA, pp. 1078–1082. [Online]. Available: http://www.
isca-speech.org/archive/Interspeech 2019/abstracts/3174.html
[18] Y. Zhang, F. Jiang, and Z. Duan, “One-class learning towards syn-
thetic voice spoofing detection,” IEEE Signal Processing Letters,
vol. 28, pp. 937–941, 2021.
[19] J. Monteiro, J. Alam, and T. H. Falk, “Generalized end-to-end
detection of spoofing attacks to automatic speaker recognizers,”
Computer Speech & Language, vol. 63, p. 101096, 2020.
[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770–778.
[21] Z. Zhang, X. Yi, and X. Zhao, “Fake speech detection using resid-
ual network with transformer encoder,” in Proceedings of the 2021
ACM Workshop on Information Hiding and Multimedia Security,
2021, pp. 13–22.
[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30, 2017.
[23] H. Tak, J. Patino, M. Todisco, A. Nautsch, N. Evans, and
A. Larcher,
“End-to-End anti-spoofing with RawNet2,”
in
ICASSP 2021 - 2021 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pp. 6369–6373.
[24] M. Ravanelli and Y. Bengio, “Speaker recognition from raw wave-
form with sincnet,” in 2018 IEEE Spoken Language Technology
Workshop (SLT).
IEEE, 2018, pp. 1021–1028.
[25] W. Ge, J. Patino, M. Todisco, and N. Evans, “Raw differentiable
architecture search for speech deepfake and spoofing detection,”
arXiv preprint arXiv:2107.12212, 2021.
[26] H. Tak, J.-w. Jung, J. Patino, M. Kamble, M. Todisco, and
N. Evans, “End-to-end spectro-temporal graph attention networks
for speaker verification anti-spoofing and speech deepfake detec-
tion,” arXiv preprint arXiv:2107.12710, 2021.
[27] D. P. Kingma and J. Ba, “Adam:
A method for stochastic
optimization,” in 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, Y. Bengio and Y. LeCun,
Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6980
[28] N. M. M¨uller, F. Dieckmann, P. Czempin, R. Canals, J. Williams,
and K. B¨ottinger. Speech is Silver, Silence is Golden: What do
ASVspoof-trained Models Really Learn?
[Online]. Available:
http://arxiv.org/abs/2106.12914
[29] T. Kinnunen, K. A. Lee, H. Delgado, N. Evans, M. Todisco,
M. Sahidullah, J. Yamagishi, and D. A. Reynolds, “t-DCF: a de-
tection cost function for the tandem assessment of spoofing coun-
termeasures and automatic speaker verification,” in Odyssey 2018
The Speaker and Language Recognition Workshop.
ISCA, pp.
312–319.
[30] “tdcf
official
implementation,”
https://www.asvspoof.org/
asvspoof2019/tDCF python v1.zip, (Accessed on 03/03/2022).
[31] J. C. Brown, “Calculation of a constant q spectral transform,” The
Journal of the Acoustical Society of America, vol. 89, no. 1, pp.
425–434, 1991.
[32] S. S. Stevens, J. Volkmann, and E. B. Newman, “A scale for the
measurement of the psychological magnitude pitch,” The journal
of the acoustical society of america, vol. 8, no. 3, pp. 185–190,
1937.
[33] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Batten-
berg, and O. Nieto, “librosa: Audio and music signal analysis in
python,” in Proceedings of the 14th python in science conference,
vol. 8.
Citeseer, 2015, pp. 18–25.
[34] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,
D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright
et al., “Scipy 1.0: fundamental algorithms for scientific comput-
ing in python,” Nature methods, vol. 17, no. 3, pp. 261–272, 2020.